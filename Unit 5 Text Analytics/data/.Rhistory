max(data[,2:8])
data <- read.csv(file="Desktop/Room/2/3.csv",header = FALSE)
mean(c(data[,2],data[,3],data[,4],data[,5],data[,6],data[,7],data[,8]))
median(c(data[,2],data[,3],data[,4],data[,5],data[,6],data[,7],data[,8]))
min(data[,2:8])
max(data[,2:8])
data <- read.csv(file="Desktop/Room/2/4.csv",header = FALSE)
mean(c(data[,2],data[,3],data[,4],data[,5],data[,6],data[,7],data[,8]))
median(c(data[,2],data[,3],data[,4],data[,5],data[,6],data[,7],data[,8]))
min(data[,2:8])
max(data[,2:8])
data <- read.csv(file="Desktop/Room/2/4.csv",header = FALSE)
mean(c(data[,2],data[,3],data[,4],data[,5],data[,6],data[,7],data[,8]))
median(c(data[,2],data[,3],data[,4],data[,5],data[,6],data[,7],data[,8]))
min(data[,2:8])
max(data[,2:8])
data <- read.csv(file="Desktop/Room/5/room5-4.csv",header = FALSE)
max(data[,2])
data(state)
statedata = cbind(data.frame(state.x77), state.abb, state.area,
state.center, state.division, state.name, state.region)
str(statedata)
plot(statedata$x, statedata$y)
plot(statedata$y, statedata$x)
plot(statedata$x, statedata$y)
plot(statedata$x, statedata$y,lty=1)
?type
?plot
plot(statedata$x, statedata$y,cex = 1)
plot(statedata$x, statedata$y,cex = 2)
plot(statedata$x, statedata$y,cex = 0.1)
plot(statedata$x, statedata$y,cex = 0.5)
tapply(statedata$HS.Grad, statedata$state.region, mean)
boxplot(statedata$Murder, statedata$state.region)
boxplot(statedata$state.region, statedata$Murder)
?boxplot
boxplot(statedata$Murder ~ statedata$state.region)
northest = subset(statedata, state.region = "Northeast")
northeast = subset(statedata, state.region = "Northeast")
northeast
northeast = subset(statedata, state.region == "Northeast")
northeast
str(statedata)
stateNew = statedata[,1:8]
str(stateNew)
life.expReg = lm(Life.Exp ~ ., data=stateNew)
summary(life.expReg)
plot(statedata$Income, statedata$Life.Exp)
life.expReg2 = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + Frost,
data = statedata)
summary(life.expReg2)
life.expReg3 = lm(Life.Exp ~ Population + Income + Murder + HS.Grad + Frost,
data = statedata)
summary(life.expReg3)
life.expReg4 = lm(Life.Exp ~ Population + Murder + HS.Grad + Frost, data = statedata)
summary(life.expReg4)
cor(stateNew)
life.expReg2 = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + Frost + Area,
data = statedata)
summary(life.expReg2)
life.expReg2 = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + Frost,
data = statedata)
life.expReg3 = lm(Life.Exp ~ Population + Income + Murder + HS.Grad + Frost,
data = statedata)
life.expReg2 = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + Frost,
data = statedata)
life.expReg3 = lm(Life.Exp ~ Population + Income + Murder + HS.Grad + Frost,
data = statedata)
life.expReg4 = lm(Life.Exp ~ Population + Murder + HS.Grad + Frost, data = statedata)
summary(life.expReg4)
summary(life.expReg2)
life.expReg5 = step(life.expReg)
summary(life.expReg)
summary(life.expReg4)
sort(predict(life.expReg4))
sort(table(statedata$Life.Exp))
predict(life.expReg4)
str(statedata)
sort(table(statedata$state.name, statedata$Life.Exp))
?sort
which(statedata$Life.Exp == min(statedata$Life.Exp))
statedata[40,]
which.max(statedata$Life.Exp)
statedata[11,]
life.expReg4$residuals
sort(life.expReg4$residuals)
1 + 1
data(state)
statedata = data.frame(state.x77)
str(statedata)
linearReg = lm(Life.Exp ~ ., data = statedata)
summary(linearReg)
linear.pred = predict(linearReg)
linear.SSE = sum((linear.pred - statedata$Life.Exp)^2)
linear.SSE
linearReg2 = lm(Life.Exp ~ Population + Murder + Frost + HS.Grad, data = statedata)
summary(linearReg2)
linear.pred2 = predict(linearReg2)
linear.SSE2 = sum((linear.pred2 - statedata$Life.Exp)^2)
linear.SSE3
linear.SSE2
library(rpart)
library(rpart.plot)
cart = rpart(Life.Exp ~ ., data = statedata)
prp(cart)
cart.pred = predict(cart)
cart.SSE = sum((cart.pred - statedata$Life.Exp)^2)
cart.SSE
cart2 = rpart(Life.Exp ~ ., data = statedata, minbucket = 5)
prp(cart2)
cart.pred2 = predict(cart2)
cart.SSE2 = sum((cart.pred2 - statedata$Life.Exp)^2)
cart.SSE2
cart3 = rpart(Life.Exp ~ Area, data = statedata, minbucket = 1)
prp(cart3)
cart.pred3 = prediction(cart3)
cart.pred3 = predict(cart3)
cart.SSE3 = sum((cart.pred3 - statedata$Life.Exp)^2)
cart.SSE3
library(caret)
library(e1071)
set.seed(111)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = seq(0.01,0.5,0.01))
CV = train(Life.Exp ~ ., data = statedata, method = "rpart",
trControl = tr.control, tuneGrid = cp.grid)
CV
best.tree = CV$finalModel
prp(best.tree)
best.tree.pre = predict(best.tree)
best.tree.pred = predict(best.tree)
best.tree.SSE = sum((best.tree.pred - statedata$Life.Exp)^2)
best.tree.SSE
CV2 = train(Life.Exp ~ Area, data = statedata, method = "rpart",
trControl = tr.control, tuneGrid = cp.grid)
best.tree2 = CV2$finalModel
prp(best.tree2)
best.tree2.pred = predict(best.tree2)
best.tree2.SSE = sum((best.tree2.pred - statedata$Life.Exp)^2)
best.tree2.SSE
wiki = read.csv("Desktop/Analytics Edge/Unit 5 Text Analytics/data/wiki.csv",
stringsAsFactors = FALSE)
str(wiki)
wiki$Vandal = as.factor(wiki$Vandal)
str(wiki)
table(wiki$Vandal)
library(tm)
library(SnowballC)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, tolower)
corpusAdded = tm_map(corpusAdded, removePunctuation)
corpusAdded = tm_map(corpusAdded, removeWords, c(stopwords("english")))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded)
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
colnames(wordsAdded)
corpusRemove = Corpus(VectorSource(wiki$Removed))
corpusRemove = tm_map(corpusRemove, tolower)
corpusRemove = tm_map(corpusRemove, removePunctuation)
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemove, tolower)
corpusRemoved = tm_map(corpusRemove, removePunctuation)
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, tolower)
corpusRemoved = tm_map(corpusRemoved, removePunctuation)
corpusRemoved = tm_map(corpusRemoved)
ls("corpusRemove")
ls(corpusRemove)
corpusRemoved = tm_map(corpusRemoved, removeWords, c(stopwords("english")))
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, tolower)
corpusRemoved = tm_map(corpusRemoved, removePunctuation)
corpusRemoved = tm_map(corpusRemoved, removeWords, c(stopwords("english")))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
sparseRemoved
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
split = sample.split(wikiWords, 0.7)
split = sample.split(wikiWords$Vandal, 0.7)
train = subset(wikiWords, split == TRUE)
test = subset(wikiWords, split == FALSE)
table(test$Vandal)
618/(618+545)
library(rpart)
library(rpart.plot)
cart = rpart(Vandal ~ ., data = train, method = "class")
prp(cart)
cart.pred = predict(cart, newdata = test)
table(test$Vandal, cart.pred$test > 0.5)
table(test$Vandal, cart.pred > 0.5)
head(cart.pred)
table(test$Vandal, cart.pred[,2] > 0.5)
(618+7)/nrow(test)
table(test$Vandal, cart.pred[,2] >= 0.5)
table(test$Vandal, cart.pred[,1] >= 0.5)
cart.pred = predict(cart, newdata = test, type = "class")
table(test$Vandal, cart.pred)
table(train$Vandal, predict(cart,type="class"))
1443/nrow(train)
wikiWords2 = wikiWords
grepl("http",http1,fixed=TRUE)
grepl("http","http1",fixed=TRUE)
grepl("http","htt1",fixed=TRUE)
grepl("http","http1",fixed=FALSE)
?grepl
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE),1,0)
summary(wikiWords2)
str(wikiWords2)
table(wikiWords2$HTTP)
train2 = subset(wikiWords2, split == TRUE)
test2 = subset(wikiWords2, split == TRUE)
cart2 = rpart(Vandal ~ ., data = train, method = "class")
prp(cart2)
cart.pred2 = predict(cart2, newdata = test2, type = "class")
table(test2$Vandal, cart.pred2)
(1443 + 38)/nrow(test)
(1443 + 38)/nrow(test2)
wikiWords$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
wikiWords2$NumWordsAdded
as.matrix(dtmAdded)
as.matrix(dtmAdded)[1:10,1:10]
mean(wikiWords2$NumWordsAdded)
cart3 = rpart(Vandal ~ ., data = train, method = "class")
prp(cart3)
cart2 = rpart(Vandal ~ ., data = train2, method = "class")
prp(cart2)
cart.pred2 = predict(cart2, newdata = test2, type = "class")
table(test2$Vandal, cart.pred2)
(1411 + 171)/nrow(test2)
train3 = subset(wikiWords2, split == TRUE)
test3 = subset(wikiWords2, split == FALSE)
cart3 = rpart(Vandal ~ ., data = train3, method = "class")
prp(cart3)
cart.pred3 = predict(cart3, newdata = test3, type = "class")
table(test3$Vandal, cart.pred3)
(517 + 260)/nrow(test3)
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
train4 = subset(wikiWords3, split == TRUE)
test4 = subset(wikiWords3, split == FALSE)
cart4 = rpart(Vandal ~ ., data = train4, method = "class")
prp(cart4)
cart.pred4 = predict(cart4, newdata = test4, type = "class")
table(test4$Vandal, cart.pred4)
(572 + 285)/nrow(test4)
wiki = read.csv("Desktop/Analytics Edge/Unit 5 Text Analytics/data/wiki.csv",
stringsAsFactors = FALSE)
wiki$Vandal = as.factor(wiki$Vandal)
table(wiki$Vandal)
library(tm)
library(SnowballC)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, tolower)
corpusAdded = tm_map(corpusAdded, removePunctuation)
corpusAdded = tm_map(corpusAdded, removeWords, c(stopwords("english")))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
# bags of word for remove
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, tolower)
corpusRemoved = tm_map(corpusRemoved, removePunctuation)
corpusRemoved = tm_map(corpusRemoved, removeWords, c(stopwords("english")))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
split = sample.split(wikiWords$Vandal, 0.7)
train = subset(wikiWords, split == TRUE)
test = subset(wikiWords, split == FALSE)
table(test$Vandal)
library(rpart)
library(rpart.plot)
cart = rpart(Vandal ~ ., data = train, method = "class")
prp(cart)
cart.pred = predict(cart, newdata = test, type = "class")
table(test$Vandal, cart.pred)
(618+12)/nrow(test)
(618+12)/(533+618+12)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE),1,0)
table(wikiWords2$HTTP)
train2 = subset(wikiWords2, split == TRUE)
test2 = subset(wikiWords2, split == TRUE)
cart2 = rpart(Vandal ~ ., data = train2, method = "class")
prp(cart2)
cart.pred2 = predict(cart2, newdata = test2, type = "class")
table(test2$Vandal, cart.pred2)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE),1,0)
table(wikiWords2$HTTP)
train2 = subset(wikiWords2, split == TRUE)
test2 = subset(wikiWords2, split == FALSE)
cart2 = rpart(Vandal ~ ., data = train2, method = "class")
prp(cart2)
cart.pred2 = predict(cart2, newdata = test2, type = "class")
table(test2$Vandal, cart.pred2)
(609 + 57)/(609+9+488+57)
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
mean(wikiWords2$NumWordsAdded)
train3 = subset(wikiWords2, split == TRUE)
test3 = subset(wikiWords2, split == FALSE)
cart3 = rpart(Vandal ~ ., data = train3, method = "class")
prp(cart3)
cart.pred3 = predict(cart3, newdata = test3, type = "class")
table(test3$Vandal, cart.pred3)
(514 + 248)/(514 + 248 + 104+297)
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
train4 = subset(wikiWords3, split == TRUE)
test4 = subset(wikiWords3, split == FALSE)
cart4 = rpart(Vandal ~ ., data = train4, method = "class")
prp(cart4)
cart.pred4 = predict(cart4, newdata = test4, type = "class")
table(test4$Vandal, cart.pred4)
library(caret)
library(e1071)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = seq(0.01, 0.5, 0.01))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = seq(0, 1, 0.1))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
cp.grid = expand.grid(.cp = seq(0, 0.3, 0.1))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
seq(0, 0.3, 0.1)
cp.grid = expand.grid(.cp = seq(0, 0.1, 0.01))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
?train
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control)
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control)
str(train4)
tr.control
cp.grid = expand.grid(.cp = seq(0, 1, 0.01))
cp.grid = expand.grid(.cp = seq(0, 1, 0.01))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,tuneGrid = cp.grid)
str(train4)
library(randomForest)
randomForest = randomForest(Vandal ~ ., data = train4, nodsize = 20, ntree = 200)
colnames(train4)
colnames(wikiTrain3) = make.names(colnames(wikiTrain3), unique = TRUE)
colnames(train4) = make.names(colnames(train4), unique = TRUE)
colnames(train4)
randomForest = randomForest(Vandal ~ ., data = train4, nodsize = 20, ntree = 200)
colnames(test4) = make.names(colnames(test4), unique = TRUE)
randomForest.pred = predict(randomForest, newdata = test4, type = "class")
table(test4$Vandal, randomForest.pred)
(582+276)/(582+276+36+269)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = seq(0, 1, 0.01))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
best.cart = CV$finalModel
CV
best.cart = CV$finalModel
best.cart.pred = predict(best.cart, newdata = test4, type = "class")
table(test4$Vandal, best.cart.pred)
(536+311)/(536+311+82+234)
log = glm(Vandal ~ ., data = train, family = binomial)
summary(log)
log = glm(Vandal ~ ., data = train4, family = binomial)
log = glm(Vandal ~ ., data = train4, family = binomial)
log.pred = predict(log, newdata = test4, type = "response")
table(test4$Vandal, log.pred > 0.5)
(494 + 309)/(124 + 236)
(494 + 309)/(124 + 236 + 494 + 309)
setwd("Desktop/Analytics Edge/Unit 5 Text Analytics/data/")
trials = read.csv("clinical_trial.csv")
str(trials)
max(nchar(trials$abstract))
trials = read.csv("clinical_trial.csv", stringsAsFactors = FALSE)
str(trials)
max(nchar(trials$abstract))
table(nchar(trials$abstract))
which.min(nchar(trials$title))
trials$title[1258]
library(tm)
library(SnowballC)
corpusTitle = Corpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
?PlainTextDocument
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, c(stopwords("english")))
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusTitle = Corpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, c(stopwords("english")))
corpusTitle = tm_map(corpusTitle, stemDocument)
library(tm)
library(SnowballC)
corpusTitle = Corpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
corpusTitle = Corpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle[[1]]
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusTitle[[1]]
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusTitle[[1]]
str(title)
str(trials)
corpusTitle1 = tm_map(corpusTitle[[1]], stemDocument)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusTitle = Corpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusTitle1 = tm_map(corpusTitle[[1]], stemDocument)
tm_map("retired", stemDocument)
corpusTitle = Corpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, tolower)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
sparseTitle = removeSparseTerms(dtmTitle, 0.95)
dtmTitle = as.data.frame(as.matrix(sparseTitle))
corpusAbstrat = Corpus(VectorSource(trials$abstract))
corpusAbstrat = tm_map(corpusAbstrat, tolower)
corpusAbstrat = tm_map(corpusAbstrat, removePunctuation)
corpusAbstrat = tm_map(corpusAbstrat, removeWords, stopwords("english"))
corpusAbstrat = tm_map(corpusAbstrat, stemDocument)
dtmAbstract = DocumentTermMatrix(corpusAbstrat)
sparseAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmAbstract = as.data.frame(as.matrix(sparseAbstract))
col(dtmTitle)
ncol(dtmTitle)
ncol(dtmAbstract)
colSums(dtmAbstract)
sort(colSums(dtmAbstract)))
sort(colSums(dtmAbstract))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
dtm = cbind(dtmTitle, dtmAbstract)
str(dtm)
dtm$trial = trials$trial
ncol(dtm)
library(caTools)
split = sample.split(dtm$trial, SplitRatio = 0.7)
train = subset(dtm, split == TRUE)
test = subset(dtm, split == FALSE)
table(train$trial)
library(caTools)
set.seed(144)
split = sample.split(dtm$trial, SplitRatio = 0.7)
train = subset(dtm, split == TRUE)
test = subset(dtm, split == FALSE)
table(train$trial)
730/nrow(train)
library(rpart)
library(rpart.plot)
trialCART = rpart(trial ~ ., data = train, method = "class")
prp(trialCART)
trainPred = predict(trialCART)[,2]
max(trainPred)
table(train$trial, trainPred > 0.5)
(631 + 441)/nrow(train)
441/(441 + 131)
631/(631 + 99)
testPred = predict(trialCART, newdata = test)[,2]
table(test$trial, testPred > 0.5)
(261 + 162)/nrow(test)
dig(table(test$trial, testPred > 0.5))
diag(table(test$trial, testPred > 0.5))
sum(diag(table(test$trial, testPred > 0.5)))/nrow(test$trial)
sum(diag(table(test$trial, testPred > 0.5)))
sum(diag(table(test$trial, testPred > 0.5)))/nrow(test)
library(ROCR)
testROCR = prediction(testPred, test$trial)
testAUC = as.numeric(performance(testROCR, "auc")@y.values)
testAUC
