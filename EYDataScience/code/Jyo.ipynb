{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# regression for speed\n",
    "Author: Bujie Xu\n",
    "\n",
    "speed = Distance/measured time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # learning curver\n",
    "# from sklearn.model_selection import learning_curve\n",
    "# from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "# def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "#                         n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "#     \"\"\"\n",
    "#     Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "#         An object of that type which is cloned for each validation.\n",
    "\n",
    "#     title : string\n",
    "#         Title for the chart.\n",
    "\n",
    "#     X : array-like, shape (n_samples, n_features)\n",
    "#         Training vector, where n_samples is the number of samples and\n",
    "#         n_features is the number of features.\n",
    "\n",
    "#     y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "#         Target relative to X for classification or regression;\n",
    "#         None for unsupervised learning.\n",
    "\n",
    "#     ylim : tuple, shape (ymin, ymax), optional\n",
    "#         Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "#     cv : int, cross-validation generator or an iterable, optional\n",
    "#         Determines the cross-validation splitting strategy.\n",
    "#         Possible inputs for cv are:\n",
    "#           - None, to use the default 3-fold cross-validation,\n",
    "#           - integer, to specify the number of folds.\n",
    "#           - An object to be used as a cross-validation generator.\n",
    "#           - An iterable yielding train/test splits.\n",
    "\n",
    "#         For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "#         :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "#         or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "#         Refer :ref:`User Guide <cross_validation>` for the various\n",
    "#         cross-validators that can be used here.\n",
    "\n",
    "#     n_jobs : integer, optional\n",
    "#         Number of jobs to run in parallel (default 1).\n",
    "#     \"\"\"\n",
    "#     plt.figure()\n",
    "#     plt.title(title)\n",
    "#     if ylim is not None:\n",
    "#         plt.ylim(*ylim)\n",
    "#     plt.xlabel(\"Training examples\")\n",
    "#     plt.ylabel(\"Score\")\n",
    "#     train_sizes, train_scores, test_scores = learning_curve(\n",
    "#         estimator, X, y, scoring=\"neg_mean_squared_error\",cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#     test_scores_std = np.std(test_scores, axis=1)\n",
    "#     plt.grid()\n",
    "\n",
    "#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.1,\n",
    "#                      color=\"r\")\n",
    "#     plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "#                      test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "#     plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "#              label=\"Training score\")\n",
    "#     plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "#              label=\"Cross-validation score\")\n",
    "\n",
    "#     plt.legend(loc=\"best\")\n",
    "#     return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # validation curve\n",
    "# from sklearn.model_selection import validation_curve\n",
    "\n",
    "# def plot_validation_curve(estimator, title, X, y,cv=None, \n",
    "#                           n_jobs = 1,param_range=np.logspace(-6, -1, 100)):\n",
    "#     train_scores, test_scores = validation_curve(\n",
    "#         estimator, X, y, param_name=\"alpha\", param_range=param_range,\n",
    "#         cv=cv, scoring=\"neg_mean_squared_error\", n_jobs = n_jobs )\n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#     test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel(\"alpha\")\n",
    "#     plt.ylabel(\"Score\")\n",
    "#     lw = 2\n",
    "#     plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "#                  color=\"darkorange\", lw=lw)\n",
    "#     plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.2,\n",
    "#                      color=\"darkorange\", lw=lw)\n",
    "#     plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "#                  color=\"navy\", lw=lw)\n",
    "#     plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "#                      test_scores_mean + test_scores_std, alpha=0.2,\n",
    "#                      color=\"navy\", lw=lw)\n",
    "#     plt.legend(loc=\"best\")\n",
    "#     return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "train_csv = pd.read_csv(filepath_or_buffer=\"../data/data_train.csv\",sep=\";\")\n",
    "test_csv = pd.read_csv(filepath_or_buffer=\"../data/data_test.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed: predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xbj/anaconda/lib/python2.7/site-packages/pandas/core/generic.py:2773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "speed_test = test_csv.loc[test_csv.Speed.isnull()]\n",
    "speed_test.Speed = speed_test.Distance/speed_test.Measured_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speed_result = speed_test[['Id','Speed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6.063448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5.485714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.646154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id      Speed\n",
       "0   1  10.134000\n",
       "1   2   6.063448\n",
       "2   3  12.037037\n",
       "3   4   5.485714\n",
       "4   5   6.646154"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speed_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2 :Regression for Candence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (Cadence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat((train_csv, test_csv))\n",
    "all_data.Speed = all_data.Speed.fillna(all_data.Distance/all_data.Measured_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data.Date = pd.to_datetime(all_data.Date)\n",
    "all_data.Time = pd.to_datetime(all_data.Time)\n",
    "all_data['Year'] = all_data.Date.apply(lambda date:date.year)\n",
    "all_data['Month'] = all_data.Date.apply(lambda date:date.month)\n",
    "all_data['Hour'] = all_data.Time.apply(lambda time:time.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove useless columns\n",
    "cadence = all_data.drop(['Index','Date','Time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change categorical data to dummys\n",
    "cadence.RiderID = cadence.RiderID.astype('category')\n",
    "cadence.Month = cadence.Month.astype('category')\n",
    "cadence.Hour = cadence.Hour.astype('category')\n",
    "cadence.Year = cadence.Year.astype('category')\n",
    "cadence = pd.get_dummies(cadence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add new feature speed/dis\n",
    "cadence['1/Measured_time'] = 1/cadence.Measured_time\n",
    "cadence['Speed^2'] = cadence.Speed ** 2\n",
    "cadence['Speed^3'] = cadence.Speed ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Average_Gradient', u'Average_heart_rate', u'Cadence', u'Distance',\n",
       "       u'Highest_point', u'Id', u'Lowest_point', u'Max_Gradient',\n",
       "       u'Max_heart_rate', u'Measured_time', u'Moving_time', u'Power', u'Speed',\n",
       "       u'RiderID_1', u'RiderID_2', u'RiderID_3', u'RiderID_4', u'RiderID_5',\n",
       "       u'RiderID_6', u'RiderID_7', u'RiderID_8', u'RiderID_9', u'RiderID_10',\n",
       "       u'RiderID_11', u'RiderID_12', u'RiderID_13', u'RiderID_14',\n",
       "       u'RiderID_15', u'Year_2008', u'Year_2009', u'Year_2010', u'Year_2011',\n",
       "       u'Year_2012', u'Year_2013', u'Year_2014', u'Year_2015', u'Year_2016',\n",
       "       u'Month_1', u'Month_2', u'Month_3', u'Month_4', u'Month_5', u'Month_6',\n",
       "       u'Month_7', u'Month_8', u'Month_9', u'Month_10', u'Month_11',\n",
       "       u'Month_12', u'Hour_0', u'Hour_1', u'Hour_2', u'Hour_3', u'Hour_4',\n",
       "       u'Hour_5', u'Hour_6', u'Hour_7', u'Hour_8', u'Hour_9', u'Hour_10',\n",
       "       u'Hour_11', u'Hour_12', u'Hour_13', u'Hour_14', u'Hour_15', u'Hour_16',\n",
       "       u'Hour_17', u'Hour_18', u'Hour_19', u'Hour_20', u'Hour_21', u'Hour_22',\n",
       "       u'Hour_23', u'1/Measured_time', u'Speed^2', u'Speed^3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadence.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cadence_train = cadence.dropna(subset=['Cadence','Power'],how = 'any')\n",
    "cadence_test = cadence.loc[cadence['Cadence'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropList = ['Cadence','Id']\n",
    "cadence_X_train, cadence_y = cadence_train.drop(dropList, axis=1), cadence_train['Cadence']\n",
    "cadence_columns = cadence_X_train.columns\n",
    "cadence_X_test = cadence_test.drop(dropList, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 74)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadence_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try log transform the target\n",
    "# numeric_feats = cadence_train.dtypes[cadence_train.dtypes != \"object\"].index\n",
    "\n",
    "#skewed_feats = cadence_train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "#skewed_feats = skewed_feats[skewed_feats > 1]\n",
    "#skewed_feats = skewed_feats.index\n",
    "#skewed_feats = np.hstack((skewed_feats[1],skewed_feats[-1]))\n",
    "#cadence_train[skewed_feats] = np.log1p(cadence_train[skewed_feats])\n",
    "#cadence_train[['Distance','/Dist']] = np.log1p(cadence_train[['Distance','1/Dist']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# min_max = MinMaxScaler()\n",
    "# X_train = min_max.fit_transform(X_train)\n",
    "# X_test = min_max.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# x_scaler = StandardScaler()\n",
    "# X_train = x_scaler.fit_transform(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train = np.hstack((X_train, riderID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadence: Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, Lasso\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# def rmse_cv(model):\n",
    "#     rmse = np.sqrt(-cross_val_score(model, X_train, y, scoring = \"neg_mean_squared_error\", cv = 5))\n",
    "#     return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_es' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5c069472d6c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrfr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_es\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# param_grid = {'n_estimators': [1000], 'max_features': [0.2]}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# cadence_model_randomForestRegressor = GridSearchCV(estimator=rfr, param_grid=param_grid,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_es' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rfr = RandomForestRegressor(n_estimators=1000, max_features=0.2)\n",
    "# param_grid = {'n_estimators': [1000], 'max_features': [0.2]}\n",
    "# cadence_model_randomForestRegressor = GridSearchCV(estimator=rfr, param_grid=param_grid, \n",
    "#                                                    n_jobs=-1, cv=5, verbose=20, scoring=\"neg_mean_squared_error\")\n",
    "cadence_model_randomForestRegressor.fit(cadence_X_train,cadence_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=0.2, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadence_model_randomForestRegressor.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-116.33974846749757"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadence_model_randomForestRegressor.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 0.2, 'n_estimators': 1000}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadence_model_randomForestRegressor.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cadence: xgboost model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBRegressor in module xgboost.sklearn:\n",
      "\n",
      "class XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      " |  Implementation of the scikit-learn API for XGBoost regression.\n",
      " |      Parameters\n",
      " |  ----------\n",
      " |  max_depth : int\n",
      " |      Maximum tree depth for base learners.\n",
      " |  learning_rate : float\n",
      " |      Boosting learning rate (xgb's \"eta\")\n",
      " |  n_estimators : int\n",
      " |      Number of boosted trees to fit.\n",
      " |  silent : boolean\n",
      " |      Whether to print messages while running boosting.\n",
      " |  objective : string or callable\n",
      " |      Specify the learning task and the corresponding learning objective or\n",
      " |      a custom objective function to be used (see note below).\n",
      " |  nthread : int\n",
      " |      Number of parallel threads used to run xgboost.\n",
      " |  gamma : float\n",
      " |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |  min_child_weight : int\n",
      " |      Minimum sum of instance weight(hessian) needed in a child.\n",
      " |  max_delta_step : int\n",
      " |      Maximum delta step we allow each tree's weight estimation to be.\n",
      " |  subsample : float\n",
      " |      Subsample ratio of the training instance.\n",
      " |  colsample_bytree : float\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |  colsample_bylevel : float\n",
      " |      Subsample ratio of columns for each split, in each level.\n",
      " |  reg_alpha : float (xgb's alpha)\n",
      " |      L1 regularization term on weights\n",
      " |  reg_lambda : float (xgb's lambda)\n",
      " |      L2 regularization term on weights\n",
      " |  scale_pos_weight : float\n",
      " |      Balancing of positive and negative weights.\n",
      " |  \n",
      " |  base_score:\n",
      " |      The initial prediction score of all instances, global bias.\n",
      " |  seed : int\n",
      " |      Random number seed.\n",
      " |  missing : float, optional\n",
      " |      Value in the data which needs to be present as a missing value. If\n",
      " |      None, defaults to np.nan.\n",
      " |  \n",
      " |  Note\n",
      " |  ----\n",
      " |  A custom objective function can be provided for the ``objective``\n",
      " |  parameter. In this case, it should have the signature\n",
      " |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |  y_true: array_like of shape [n_samples]\n",
      " |      The target values\n",
      " |  y_pred: array_like of shape [n_samples]\n",
      " |      The predicted values\n",
      " |  \n",
      " |  grad: array_like of shape [n_samples]\n",
      " |      The value of the gradient for each sample point.\n",
      " |  hess: array_like of shape [n_samples]\n",
      " |      The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBRegressor\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', nthread=-1, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, seed=0, missing=None)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  apply(self, X, ntree_limit=0)\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  booster(self)\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  evals_result(self)\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If eval_set is passed to the `fit` function, you can call evals_result() to\n",
      " |      get evaluation results for all passed eval_sets. When eval_metric is also\n",
      " |      passed to the `fit` function, the evals_result will contain the eval_metrics\n",
      " |      passed to the `fit` function\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |      clf = xgb.XGBModel(**param_dist)\n",
      " |      \n",
      " |      clf.fit(X_train, y_train,\n",
      " |              eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |              eval_metric='logloss',\n",
      " |              verbose=True)\n",
      " |      \n",
      " |      evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable evals_result will contain:\n",
      " |      {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |       'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X, y, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True)\n",
      " |      Fit the gradient boosting model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) tuple pairs to use as a validation set for\n",
      " |          early-stopping\n",
      " |      eval_metric : str, callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use. See\n",
      " |          doc/parameter.md. If callable, a custom evaluation metric. The call\n",
      " |          signature is func(y_predicted, y_true) where y_true will be a\n",
      " |          DMatrix object such that you may need to call the get_label\n",
      " |          method. It must return a str, value pair where the str is a name\n",
      " |          for the evaluation and value is the value of the evaluation\n",
      " |          function. This objective is always minimized.\n",
      " |      early_stopping_rounds : int\n",
      " |          Activates early stopping. Validation error needs to decrease at\n",
      " |          least every <early_stopping_rounds> round(s) to continue training.\n",
      " |          Requires at least one item in evals.  If there's more than one,\n",
      " |          will use the last. Returns the model from the last iteration\n",
      " |          (not the best one). If early stopping occurs, the model will\n",
      " |          have three additional fields: bst.best_score, bst.best_iteration\n",
      " |          and bst.best_ntree_limit.\n",
      " |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      " |          and/or num_class appears in the parameters)\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      " |          metric measured on the validation set to stderr.\n",
      " |  \n",
      " |  get_params(self, deep=False)\n",
      " |      Get parameter.s\n",
      " |  \n",
      " |  get_xgb_params(self)\n",
      " |      Get xgboost type parameters.\n",
      " |  \n",
      " |  predict(self, data, output_margin=False, ntree_limit=0)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from XGBModel:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      Best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.XGBRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from sklearn import cross_validation\n",
    "# cadence_X_dtrain, cadence_X_deval, cadence_y_dtrain, cadence_y_deval = \\\n",
    "#     cross_validation.train_test_split(cadence_X_train, cadence_y, random_state = 1026, test_size=0.3)\n",
    "# dtrain = xgb.DMatrix(cadence_X_dtrain, cadence_y_dtrain)\n",
    "# deval = xgb.DMatrix(cadence_X_deval, cadence_y_deval)\n",
    "# watchlist = [(deval, 'eval')]\n",
    "# params = {\n",
    "#     'booster': 'gbtree',\n",
    "#     'objective': 'reg:linear',\n",
    "#     'subsample': 0.7,\n",
    "#     'colsample_bytree': 0.85,\n",
    "#     'eta': 0.01,\n",
    "#     'max_depth': 7,\n",
    "#     'seed': 2016,\n",
    "#     'silent': 0,\n",
    "#     'eval_metric': 'rmse'\n",
    "# }\n",
    "# clf = xgb.train(params, dtrain, 1000, watchlist, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cadence: AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "rd = DecisionTreeRegressor(max_depth=7)\n",
    "clf = AdaBoostRegressor(n_estimators=500,base_estimator=rd, learning_rate = 0.5, random_state= 2017)\n",
    "scores = cross_val_score(clf, cadence_X_train, cadence_y, scoring = \"neg_mean_squared_error\", n_jobs=2)\n",
    "# param_grid = {'n_estimators': [1000], 'max_features': [0.2]}\n",
    "# cadence_model_randomForestRegressor = GridSearchCV(estimator=rfr, param_grid=param_grid, \n",
    "#                                                    n_jobs=-1, cv=5, verbose=20, scoring=\"neg_mean_squared_error\")\n",
    "# cadence_model_AdaBoost.fit(cadence_X_train,cadence_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-653.71006308, -785.60322364, -513.625924  ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict (Cadence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Cadence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>30001.0</td>\n",
       "      <td>73.4630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30001</th>\n",
       "      <td>30002.0</td>\n",
       "      <td>98.0581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30002</th>\n",
       "      <td>30003.0</td>\n",
       "      <td>93.4522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30003</th>\n",
       "      <td>30004.0</td>\n",
       "      <td>89.3409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30004</th>\n",
       "      <td>30005.0</td>\n",
       "      <td>71.1088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  Cadence\n",
       "30000  30001.0  73.4630\n",
       "30001  30002.0  98.0581\n",
       "30002  30003.0  93.4522\n",
       "30003  30004.0  89.3409\n",
       "30004  30005.0  71.1088"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadence_test.Cadence = cadence_model_randomForestRegressor.predict(cadence_X_test)\n",
    "cadence_result = cadence_test[['Id','Cadence']]\n",
    "cadence_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cadence_result.to_csv(path_or_buf=\"../submit/submit4_cadence.csv\", sep=\";\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3: Power: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "power = cadence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average_Gradient</th>\n",
       "      <th>Average_heart_rate</th>\n",
       "      <th>Cadence</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Highest_point</th>\n",
       "      <th>Id</th>\n",
       "      <th>Lowest_point</th>\n",
       "      <th>Max_Gradient</th>\n",
       "      <th>Max_heart_rate</th>\n",
       "      <th>Measured_time</th>\n",
       "      <th>...</th>\n",
       "      <th>Hour_17</th>\n",
       "      <th>Hour_18</th>\n",
       "      <th>Hour_19</th>\n",
       "      <th>Hour_20</th>\n",
       "      <th>Hour_21</th>\n",
       "      <th>Hour_22</th>\n",
       "      <th>Hour_23</th>\n",
       "      <th>1/Measured_time</th>\n",
       "      <th>Speed^2</th>\n",
       "      <th>Speed^3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>105.8</td>\n",
       "      <td>85.3</td>\n",
       "      <td>9980.400</td>\n",
       "      <td>128.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>114</td>\n",
       "      <td>1375</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>52.685426</td>\n",
       "      <td>382.415730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.3</td>\n",
       "      <td>157.5</td>\n",
       "      <td>88.3</td>\n",
       "      <td>1666.010</td>\n",
       "      <td>204.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>170</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>43.707315</td>\n",
       "      <td>288.955649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>158.9</td>\n",
       "      <td>103.4</td>\n",
       "      <td>345.600</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>92.160000</td>\n",
       "      <td>884.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8.3</td>\n",
       "      <td>99.1</td>\n",
       "      <td>66.4</td>\n",
       "      <td>1572.900</td>\n",
       "      <td>617.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>487.2</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>104</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>163.527954</td>\n",
       "      <td>2091.163563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7.6</td>\n",
       "      <td>100.3</td>\n",
       "      <td>60.3</td>\n",
       "      <td>435.352</td>\n",
       "      <td>54.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.4</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>111</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>54.447390</td>\n",
       "      <td>401.758984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Average_Gradient  Average_heart_rate  Cadence  Distance  Highest_point  Id  \\\n",
       "0               0.0               105.8     85.3  9980.400          128.4 NaN   \n",
       "1               3.3               157.5     88.3  1666.010          204.6 NaN   \n",
       "2               0.0               158.9    103.4   345.600           59.0 NaN   \n",
       "3              -8.3                99.1     66.4  1572.900          617.8 NaN   \n",
       "4              -7.6               100.3     60.3   435.352           54.4 NaN   \n",
       "\n",
       "   Lowest_point  Max_Gradient  Max_heart_rate  Measured_time     ...       \\\n",
       "0         124.6           5.0             114           1375     ...        \n",
       "1         147.9          16.0             170            252     ...        \n",
       "2          59.0           0.0             160             36     ...        \n",
       "3         487.2          -0.5             104            123     ...        \n",
       "4          21.4          -4.8             111             59     ...        \n",
       "\n",
       "   Hour_17  Hour_18  Hour_19  Hour_20  Hour_21  Hour_22  Hour_23  \\\n",
       "0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0   \n",
       "4        0        0        0        0        0        0        0   \n",
       "\n",
       "   1/Measured_time     Speed^2      Speed^3  \n",
       "0         0.000727   52.685426   382.415730  \n",
       "1         0.003968   43.707315   288.955649  \n",
       "2         0.027778   92.160000   884.736000  \n",
       "3         0.008130  163.527954  2091.163563  \n",
       "4         0.016949   54.447390   401.758984  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "power_train = power.dropna(subset=['Cadence','Power'],how = 'any')\n",
    "power_test = power.loc[cadence['Power'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropList = ['Power','Id']\n",
    "power_X_train, power_y = power_train.drop(dropList, axis=1), power_train['Power']\n",
    "columns = power_X_train.columns\n",
    "power_X_test = power_test.drop(dropList, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# min_max = MinMaxScaler()\n",
    "# X_train = min_max.fit_transform(X_train)\n",
    "# X_test = min_max.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 74)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power: Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power: RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] max_features=0.2, n_estimators=1000 .............................\n",
      "[CV] max_features=0.2, n_estimators=1000 .............................\n",
      "[CV] max_features=0.2, n_estimators=1000 .............................\n",
      "[CV] max_features=0.2, n_estimators=1000 .............................\n",
      "[CV]  max_features=0.2, n_estimators=1000, score=-2649.451434, total=36.7min\n",
      "[CV]  max_features=0.2, n_estimators=1000, score=-2317.414612, total=36.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed: 55.1min\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   5 | elapsed: 55.1min remaining: 82.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] max_features=0.2, n_estimators=1000 .............................\n",
      "[CV]  max_features=0.2, n_estimators=1000, score=-1899.647764, total=36.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   5 | elapsed: 61.0min remaining: 40.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_features=0.2, n_estimators=1000, score=-3262.600279, total=47.7min\n",
      "[CV]  max_features=0.2, n_estimators=1000, score=-2089.771083, total=39.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed: 98.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed: 98.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=4,\n",
       "       param_grid={'n_estimators': [1000], 'max_features': [0.2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=20)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rfr = RandomForestRegressor()\n",
    "param_grid = {'n_estimators': [1000], 'max_features': [0.2]}\n",
    "power_model_randomForest = GridSearchCV(estimator=rfr, param_grid=param_grid, n_jobs=4, cv=5, verbose=20, scoring=\"neg_mean_squared_error\")\n",
    "power_model_randomForest.fit(power_X_train, power_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=0.2, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_model_randomForest.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2443.7770342159642"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_model_randomForest.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 0.2, 'n_estimators': 1000}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_model_randomForest.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power: Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average_Gradient</th>\n",
       "      <th>Average_heart_rate</th>\n",
       "      <th>Cadence</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Highest_point</th>\n",
       "      <th>Lowest_point</th>\n",
       "      <th>Max_Gradient</th>\n",
       "      <th>Max_heart_rate</th>\n",
       "      <th>Measured_time</th>\n",
       "      <th>Moving_time</th>\n",
       "      <th>...</th>\n",
       "      <th>Hour_17</th>\n",
       "      <th>Hour_18</th>\n",
       "      <th>Hour_19</th>\n",
       "      <th>Hour_20</th>\n",
       "      <th>Hour_21</th>\n",
       "      <th>Hour_22</th>\n",
       "      <th>Hour_23</th>\n",
       "      <th>1/Measured_time</th>\n",
       "      <th>Speed^2</th>\n",
       "      <th>Speed^3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>114.8</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9824.80</td>\n",
       "      <td>71.4</td>\n",
       "      <td>25.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1906</td>\n",
       "      <td>1223</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>26.570617</td>\n",
       "      <td>136.962750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>145.8</td>\n",
       "      <td>93.2</td>\n",
       "      <td>351.68</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>85.650154</td>\n",
       "      <td>792.669636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15002</th>\n",
       "      <td>-3.7</td>\n",
       "      <td>117.0</td>\n",
       "      <td>89.6</td>\n",
       "      <td>1099.72</td>\n",
       "      <td>85.9</td>\n",
       "      <td>44.7</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>137</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>65.386250</td>\n",
       "      <td>528.724756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>1.7</td>\n",
       "      <td>121.4</td>\n",
       "      <td>93.0</td>\n",
       "      <td>574.70</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9.7</td>\n",
       "      <td>126</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>58.716460</td>\n",
       "      <td>449.924664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>112.9</td>\n",
       "      <td>93.5</td>\n",
       "      <td>1035.60</td>\n",
       "      <td>129.4</td>\n",
       "      <td>127.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>115</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>60.629055</td>\n",
       "      <td>472.086088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Average_Gradient  Average_heart_rate  Cadence  Distance  Highest_point  \\\n",
       "15000              -0.4               114.8     83.1   9824.80           71.4   \n",
       "15001               0.0               145.8     93.2    351.68           32.0   \n",
       "15002              -3.7               117.0     89.6   1099.72           85.9   \n",
       "15003               1.7               121.4     93.0    574.70           22.0   \n",
       "15004               0.0               112.9     93.5   1035.60          129.4   \n",
       "\n",
       "       Lowest_point  Max_Gradient  Max_heart_rate  Measured_time  Moving_time  \\\n",
       "15000          25.6           6.1             136           1906         1223   \n",
       "15001          32.0           0.0             150             38           38   \n",
       "15002          44.7          -0.6             137            136          136   \n",
       "15003           8.1           9.7             126             75           75   \n",
       "15004         127.0          14.4             115            133          133   \n",
       "\n",
       "          ...      Hour_17  Hour_18  Hour_19  Hour_20  Hour_21  Hour_22  \\\n",
       "15000     ...            0        0        0        0        0        0   \n",
       "15001     ...            0        0        0        0        0        0   \n",
       "15002     ...            0        0        0        0        0        0   \n",
       "15003     ...            0        0        0        0        0        0   \n",
       "15004     ...            0        0        0        0        0        0   \n",
       "\n",
       "       Hour_23  1/Measured_time    Speed^2     Speed^3  \n",
       "15000        0         0.000525  26.570617  136.962750  \n",
       "15001        0         0.026316  85.650154  792.669636  \n",
       "15002        0         0.007353  65.386250  528.724756  \n",
       "15003        0         0.013333  58.716460  449.924664  \n",
       "15004        0         0.007519  60.629055  472.086088  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def power_predict(model):\n",
    "    power_test.Power = model.predict(power_X_test)\n",
    "    power_result = power_test[['Id','Power']]\n",
    "    return power_result\n",
    "power_result = power_predict(power_model_randomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>15001.0</td>\n",
       "      <td>120.3240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15001</th>\n",
       "      <td>15002.0</td>\n",
       "      <td>165.3472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15002</th>\n",
       "      <td>15003.0</td>\n",
       "      <td>83.2832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>15004.0</td>\n",
       "      <td>142.2309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15004</th>\n",
       "      <td>15005.0</td>\n",
       "      <td>105.3928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id     Power\n",
       "15000  15001.0  120.3240\n",
       "15001  15002.0  165.3472\n",
       "15002  15003.0   83.2832\n",
       "15003  15004.0  142.2309\n",
       "15004  15005.0  105.3928"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speed_result = speed_result.rename(columns={'Speed': 'Predictor'})\n",
    "cadence_result = cadence_result.rename(columns = {'Cadence': 'Predictor'})\n",
    "power_result = power_result.rename(columns={'Power': 'Predictor'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.concat((speed_result,cadence_result,power_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.Id = result.Id.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv(path_or_buf=\"../submit/submit4.csv\", sep=\";\",index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
