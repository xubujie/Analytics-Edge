life.expReg4 = lm(Life.Exp ~ Population + Murder + HS.Grad + Frost, data = statedata)
summary(life.expReg4)
summary(life.expReg2)
life.expReg5 = step(life.expReg)
summary(life.expReg)
summary(life.expReg4)
sort(predict(life.expReg4))
sort(table(statedata$Life.Exp))
predict(life.expReg4)
str(statedata)
sort(table(statedata$state.name, statedata$Life.Exp))
?sort
which(statedata$Life.Exp == min(statedata$Life.Exp))
statedata[40,]
which.max(statedata$Life.Exp)
statedata[11,]
life.expReg4$residuals
sort(life.expReg4$residuals)
1 + 1
data(state)
statedata = data.frame(state.x77)
str(statedata)
linearReg = lm(Life.Exp ~ ., data = statedata)
summary(linearReg)
linear.pred = predict(linearReg)
linear.SSE = sum((linear.pred - statedata$Life.Exp)^2)
linear.SSE
linearReg2 = lm(Life.Exp ~ Population + Murder + Frost + HS.Grad, data = statedata)
summary(linearReg2)
linear.pred2 = predict(linearReg2)
linear.SSE2 = sum((linear.pred2 - statedata$Life.Exp)^2)
linear.SSE3
linear.SSE2
library(rpart)
library(rpart.plot)
cart = rpart(Life.Exp ~ ., data = statedata)
prp(cart)
cart.pred = predict(cart)
cart.SSE = sum((cart.pred - statedata$Life.Exp)^2)
cart.SSE
cart2 = rpart(Life.Exp ~ ., data = statedata, minbucket = 5)
prp(cart2)
cart.pred2 = predict(cart2)
cart.SSE2 = sum((cart.pred2 - statedata$Life.Exp)^2)
cart.SSE2
cart3 = rpart(Life.Exp ~ Area, data = statedata, minbucket = 1)
prp(cart3)
cart.pred3 = prediction(cart3)
cart.pred3 = predict(cart3)
cart.SSE3 = sum((cart.pred3 - statedata$Life.Exp)^2)
cart.SSE3
library(caret)
library(e1071)
set.seed(111)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = seq(0.01,0.5,0.01))
CV = train(Life.Exp ~ ., data = statedata, method = "rpart",
trControl = tr.control, tuneGrid = cp.grid)
CV
best.tree = CV$finalModel
prp(best.tree)
best.tree.pre = predict(best.tree)
best.tree.pred = predict(best.tree)
best.tree.SSE = sum((best.tree.pred - statedata$Life.Exp)^2)
best.tree.SSE
CV2 = train(Life.Exp ~ Area, data = statedata, method = "rpart",
trControl = tr.control, tuneGrid = cp.grid)
best.tree2 = CV2$finalModel
prp(best.tree2)
best.tree2.pred = predict(best.tree2)
best.tree2.SSE = sum((best.tree2.pred - statedata$Life.Exp)^2)
best.tree2.SSE
wiki = read.csv("Desktop/Analytics Edge/Unit 5 Text Analytics/data/wiki.csv",
stringsAsFactors = FALSE)
str(wiki)
wiki$Vandal = as.factor(wiki$Vandal)
str(wiki)
table(wiki$Vandal)
library(tm)
library(SnowballC)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, tolower)
corpusAdded = tm_map(corpusAdded, removePunctuation)
corpusAdded = tm_map(corpusAdded, removeWords, c(stopwords("english")))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded)
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
colnames(wordsAdded)
corpusRemove = Corpus(VectorSource(wiki$Removed))
corpusRemove = tm_map(corpusRemove, tolower)
corpusRemove = tm_map(corpusRemove, removePunctuation)
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemove, tolower)
corpusRemoved = tm_map(corpusRemove, removePunctuation)
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, tolower)
corpusRemoved = tm_map(corpusRemoved, removePunctuation)
corpusRemoved = tm_map(corpusRemoved)
ls("corpusRemove")
ls(corpusRemove)
corpusRemoved = tm_map(corpusRemoved, removeWords, c(stopwords("english")))
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, tolower)
corpusRemoved = tm_map(corpusRemoved, removePunctuation)
corpusRemoved = tm_map(corpusRemoved, removeWords, c(stopwords("english")))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
sparseRemoved
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
split = sample.split(wikiWords, 0.7)
split = sample.split(wikiWords$Vandal, 0.7)
train = subset(wikiWords, split == TRUE)
test = subset(wikiWords, split == FALSE)
table(test$Vandal)
618/(618+545)
library(rpart)
library(rpart.plot)
cart = rpart(Vandal ~ ., data = train, method = "class")
prp(cart)
cart.pred = predict(cart, newdata = test)
table(test$Vandal, cart.pred$test > 0.5)
table(test$Vandal, cart.pred > 0.5)
head(cart.pred)
table(test$Vandal, cart.pred[,2] > 0.5)
(618+7)/nrow(test)
table(test$Vandal, cart.pred[,2] >= 0.5)
table(test$Vandal, cart.pred[,1] >= 0.5)
cart.pred = predict(cart, newdata = test, type = "class")
table(test$Vandal, cart.pred)
table(train$Vandal, predict(cart,type="class"))
1443/nrow(train)
wikiWords2 = wikiWords
grepl("http",http1,fixed=TRUE)
grepl("http","http1",fixed=TRUE)
grepl("http","htt1",fixed=TRUE)
grepl("http","http1",fixed=FALSE)
?grepl
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE),1,0)
summary(wikiWords2)
str(wikiWords2)
table(wikiWords2$HTTP)
train2 = subset(wikiWords2, split == TRUE)
test2 = subset(wikiWords2, split == TRUE)
cart2 = rpart(Vandal ~ ., data = train, method = "class")
prp(cart2)
cart.pred2 = predict(cart2, newdata = test2, type = "class")
table(test2$Vandal, cart.pred2)
(1443 + 38)/nrow(test)
(1443 + 38)/nrow(test2)
wikiWords$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
wikiWords2$NumWordsAdded
as.matrix(dtmAdded)
as.matrix(dtmAdded)[1:10,1:10]
mean(wikiWords2$NumWordsAdded)
cart3 = rpart(Vandal ~ ., data = train, method = "class")
prp(cart3)
cart2 = rpart(Vandal ~ ., data = train2, method = "class")
prp(cart2)
cart.pred2 = predict(cart2, newdata = test2, type = "class")
table(test2$Vandal, cart.pred2)
(1411 + 171)/nrow(test2)
train3 = subset(wikiWords2, split == TRUE)
test3 = subset(wikiWords2, split == FALSE)
cart3 = rpart(Vandal ~ ., data = train3, method = "class")
prp(cart3)
cart.pred3 = predict(cart3, newdata = test3, type = "class")
table(test3$Vandal, cart.pred3)
(517 + 260)/nrow(test3)
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
train4 = subset(wikiWords3, split == TRUE)
test4 = subset(wikiWords3, split == FALSE)
cart4 = rpart(Vandal ~ ., data = train4, method = "class")
prp(cart4)
cart.pred4 = predict(cart4, newdata = test4, type = "class")
table(test4$Vandal, cart.pred4)
(572 + 285)/nrow(test4)
wiki = read.csv("Desktop/Analytics Edge/Unit 5 Text Analytics/data/wiki.csv",
stringsAsFactors = FALSE)
wiki$Vandal = as.factor(wiki$Vandal)
table(wiki$Vandal)
library(tm)
library(SnowballC)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, tolower)
corpusAdded = tm_map(corpusAdded, removePunctuation)
corpusAdded = tm_map(corpusAdded, removeWords, c(stopwords("english")))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
# bags of word for remove
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, tolower)
corpusRemoved = tm_map(corpusRemoved, removePunctuation)
corpusRemoved = tm_map(corpusRemoved, removeWords, c(stopwords("english")))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
split = sample.split(wikiWords$Vandal, 0.7)
train = subset(wikiWords, split == TRUE)
test = subset(wikiWords, split == FALSE)
table(test$Vandal)
library(rpart)
library(rpart.plot)
cart = rpart(Vandal ~ ., data = train, method = "class")
prp(cart)
cart.pred = predict(cart, newdata = test, type = "class")
table(test$Vandal, cart.pred)
(618+12)/nrow(test)
(618+12)/(533+618+12)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE),1,0)
table(wikiWords2$HTTP)
train2 = subset(wikiWords2, split == TRUE)
test2 = subset(wikiWords2, split == TRUE)
cart2 = rpart(Vandal ~ ., data = train2, method = "class")
prp(cart2)
cart.pred2 = predict(cart2, newdata = test2, type = "class")
table(test2$Vandal, cart.pred2)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE),1,0)
table(wikiWords2$HTTP)
train2 = subset(wikiWords2, split == TRUE)
test2 = subset(wikiWords2, split == FALSE)
cart2 = rpart(Vandal ~ ., data = train2, method = "class")
prp(cart2)
cart.pred2 = predict(cart2, newdata = test2, type = "class")
table(test2$Vandal, cart.pred2)
(609 + 57)/(609+9+488+57)
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
mean(wikiWords2$NumWordsAdded)
train3 = subset(wikiWords2, split == TRUE)
test3 = subset(wikiWords2, split == FALSE)
cart3 = rpart(Vandal ~ ., data = train3, method = "class")
prp(cart3)
cart.pred3 = predict(cart3, newdata = test3, type = "class")
table(test3$Vandal, cart.pred3)
(514 + 248)/(514 + 248 + 104+297)
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
train4 = subset(wikiWords3, split == TRUE)
test4 = subset(wikiWords3, split == FALSE)
cart4 = rpart(Vandal ~ ., data = train4, method = "class")
prp(cart4)
cart.pred4 = predict(cart4, newdata = test4, type = "class")
table(test4$Vandal, cart.pred4)
library(caret)
library(e1071)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = seq(0.01, 0.5, 0.01))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = seq(0, 1, 0.1))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
cp.grid = expand.grid(.cp = seq(0, 0.3, 0.1))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
seq(0, 0.3, 0.1)
cp.grid = expand.grid(.cp = seq(0, 0.1, 0.01))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
?train
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control)
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control)
str(train4)
tr.control
cp.grid = expand.grid(.cp = seq(0, 1, 0.01))
cp.grid = expand.grid(.cp = seq(0, 1, 0.01))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,tuneGrid = cp.grid)
str(train4)
library(randomForest)
randomForest = randomForest(Vandal ~ ., data = train4, nodsize = 20, ntree = 200)
colnames(train4)
colnames(wikiTrain3) = make.names(colnames(wikiTrain3), unique = TRUE)
colnames(train4) = make.names(colnames(train4), unique = TRUE)
colnames(train4)
randomForest = randomForest(Vandal ~ ., data = train4, nodsize = 20, ntree = 200)
colnames(test4) = make.names(colnames(test4), unique = TRUE)
randomForest.pred = predict(randomForest, newdata = test4, type = "class")
table(test4$Vandal, randomForest.pred)
(582+276)/(582+276+36+269)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = seq(0, 1, 0.01))
CV = train(Vandal ~ ., data = train4, method = "rpart", trControl = tr.control,
tuneGrid = cp.grid)
best.cart = CV$finalModel
CV
best.cart = CV$finalModel
best.cart.pred = predict(best.cart, newdata = test4, type = "class")
table(test4$Vandal, best.cart.pred)
(536+311)/(536+311+82+234)
log = glm(Vandal ~ ., data = train, family = binomial)
summary(log)
log = glm(Vandal ~ ., data = train4, family = binomial)
log = glm(Vandal ~ ., data = train4, family = binomial)
log.pred = predict(log, newdata = test4, type = "response")
table(test4$Vandal, log.pred > 0.5)
(494 + 309)/(124 + 236)
(494 + 309)/(124 + 236 + 494 + 309)
setwd("Desktop/Analytics Edge/Unit 6 Clustering/data/")
stocks = read.csv("StocksCluster.csv")
str(stocks)
summary(stocks)
table(stocks$PositiveDec)
mean(stocks$PositiveDec)
cor(stocks)
max(cor(stocks))
summary(stocks)
sort(mean(stocks))
apply(stocks, mean)
tapply(stocks, mean)
dapply(stocks, mean)
?apply
summary(stocks)
library(caTools)
set.seed(144)
spl = sample.split(stocks$PositiveDec, SplitRatio = 0.7)
stocksTrain = subset(stocks, spl == TRUE)
stocksTest = subset(stocks, spl == FALSE)
StocksModel = glm(PositiveDec ~ ., data = stocksTrain, family = binomial)
trianPred = predict(StocksModel,type = "response")
trainPred = predict(StocksModel,type = "response")
table(stocksTrain$PositiveDec, trainPred > 0,5)
table(stocksTrain$PositiveDec, trainPred > 0.5)
(990 + 3640)/nrow(stocksTrain)
testPred = predict(StocksModel, newdata = stocksTest, type = "response")
table(stocksTest$PositiveDec, testPred > 0,5)
table(stocksTest$PositiveDec, testPred > 0.5)
(417 + 1553)/nrow(test)
(417 + 1553)/nrow(stocksTest)
table(stocksTest$PositiveDec)
1897/nrow(stocksTest)
limitedTrain = stocksTrain
limitedTrain$PositiveDec = NULL
limitedTest = stocksTest
limitedTest$PositiveDec = NULL
library(caret)
preproc = preProcess(limitedTrain)
normTrain = predict(preproc, limitedTrain)
normTest = predict(preproc, limitedTest)
summary(normTrain)
mean(normTrain$ReturnJan)
set.seed(144)
km = kmeans(normTrain, centers = 3)
str(km)
table(km$cluster)
library(flexclust)
install.packages("flexclust")
library(flexclust)
km.kcca = as.kcca(km, normTrain)
clusterTrain = predict(km.kcca)
clusterTest = predict(km.kcca, newdata = normTest)
str(clusterTest)
table(clusterTest)
stocksTrain = subset(stocksTrain, clusterTrain == 1)
stocksTrain1 = subset(stocksTrain, clusterTrain == 1)
stocksTrain2 = subset(stocksTrain, clusterTrain == 2)
stocksTrain3 = subset(stocksTrain, clusterTrain == 3)
stocksTest1 = subset(stocksTest, clusterTest == 1)
stocksTest2 = subset(stocksTest, clusterTest == 2)
stocksTest3 = subset(stocksTest, clusterTest == 3)
mean(stocksTrain1$PositiveDec)
str(stocksTrain1)
summary(stocksTrain1)
stocksTrain1 = subset(stocksTrain, clusterTrain == 1)
summary(stocksTrain1$PositiveDec)
summary(stocksTrain2$PositiveDec)
summary(stocksTrain3$PositiveDec)
summary(stocksTrain2)
summary(stocksTrain3)
summary(stocksTest1)
summary(stocksTest2)
summary(stocksTest3)
StocksModel1 = glm(PositiveDec ~ ., data = stocksTrian1, family = binomial)
StocksModel1 = glm(PositiveDec ~ ., data = stocksTrain1, family = binomial)
StocksModel2 = glm(PositiveDec ~ ., data = stocksTrain2, family = binomial)
StocksModel3 = glm(PositiveDec ~ ., data = stocksTrain3, family = binomial)
summary(StocksModel1)
summary(StocksModel2)
summary(StocksModel3)
?subset
na.omit(stocksTrain1)
stocksTrain1 = na.omit(stocksTrain1)
summary(stocksTrain1)
stocksTrain2 = na.omit(stocksTrain2)
stocksTrain3 = na.omit(stocksTrain3)
StocksModel1 = glm(PositiveDec ~ ., data = stocksTrain1, family = binomial)
StocksModel2 = glm(PositiveDec ~ ., data = stocksTrain2, family = binomial)
StocksModel3 = glm(PositiveDec ~ ., data = stocksTrain3, family = binomial)
summary(StocksModel1)
summary(StocksModel2)
summary(StocksModel3)
PredictTest1 = predict(StocksModel1, newdata = stocksTest1, type = "response")
table(stocksTest1$PositiveDec, PredictTest1 > 0.5)
(48 + 742)/nrow(stocksTest1)
(48 + 742)/(48+742+453+55)
table(clusterTest)
table(km$cluster)
set.seed(144)
km = kmeans(normTrain, centers = 3)
table(km$cluster)
km.kcca = as.kcca(km, normTrain)
clusterTrain = predict(km.kcca)
clusterTest = predict(km.kcca, newdata = normTest)
str(clusterTest)
table(clusterTest)
stocksTrain1 = subset(stocksTrain, clusterTrain == 1)
stocksTrain2 = subset(stocksTrain, clusterTrain == 2)
stocksTrain3 = subset(stocksTrain, clusterTrain == 3)
stocksTrain1 = na.omit(stocksTrain1)
stocksTrain2 = na.omit(stocksTrain2)
stocksTrain3 = na.omit(stocksTrain3)
stocksTest1 = subset(stocksTest, clusterTest == 1)
stocksTest2 = subset(stocksTest, clusterTest == 2)
stocksTest3 = subset(stocksTest, clusterTest == 3)
StocksModel1 = glm(PositiveDec ~ ., data = stocksTrain1, family = binomial)
StocksModel2 = glm(PositiveDec ~ ., data = stocksTrain2, family = binomial)
StocksModel3 = glm(PositiveDec ~ ., data = stocksTrain3, family = binomial)
PredictTest1 = predict(StocksModel1, newdata = stocksTest1, type = "response")
table(stocksTest1$PositiveDec, PredictTest1 > 0.5)
stocksTrain1 = subset(stocksTrain, clusterTrain == 1)
stocksTrain2 = subset(stocksTrain, clusterTrain == 2)
stocksTrain3 = subset(stocksTrain, clusterTrain == 3)
stocksTest1 = subset(stocksTest, clusterTest == 1)
stocksTest2 = subset(stocksTest, clusterTest == 2)
stocksTest3 = subset(stocksTest, clusterTest == 3)
StocksModel1 = glm(PositiveDec ~ ., data = stocksTrain1, family = binomial)
StocksModel2 = glm(PositiveDec ~ ., data = stocksTrain2, family = binomial)
StocksModel3 = glm(PositiveDec ~ ., data = stocksTrain3, family = binomial)
PredictTest1 = predict(StocksModel1, newdata = stocksTest1, type = "response")
table(stocksTest1$PositiveDec, PredictTest1 > 0.5)
set.seed(144)
spl = sample.split(stocks$PositiveDec, SplitRatio = 0.7)
stocksTrain = subset(stocks, spl == TRUE)
stocksTest = subset(stocks, spl == FALSE)
# logistic
StocksModel = glm(PositiveDec ~ ., data = stocksTrain, family = binomial)
trainPred = predict(StocksModel,type = "response")
table(stocksTrain$PositiveDec, trainPred > 0.5)
testPred = predict(StocksModel, newdata = stocksTest, type = "response")
table(stocksTest$PositiveDec, testPred > 0.5)
# Clustering
limitedTrain = stocksTrain
limitedTrain$PositiveDec = NULL
limitedTest = stocksTest
limitedTest$PositiveDec = NULL
## normalize
library(caret)
preproc = preProcess(limitedTrain)
normTrain = predict(preproc, limitedTrain)
normTest = predict(preproc, limitedTest)
set.seed(144)
km = kmeans(normTrain, centers = 3)
table(km$cluster)
library(flexclust)
km.kcca = as.kcca(km, normTrain)
clusterTrain = predict(km.kcca)
clusterTest = predict(km.kcca, newdata = normTest)
str(clusterTest)
table(clusterTest)
# cluster-specific-predictions
stocksTrain1 = subset(stocksTrain, clusterTrain == 1)
stocksTrain2 = subset(stocksTrain, clusterTrain == 2)
stocksTrain3 = subset(stocksTrain, clusterTrain == 3)
stocksTest1 = subset(stocksTest, clusterTest == 1)
stocksTest2 = subset(stocksTest, clusterTest == 2)
stocksTest3 = subset(stocksTest, clusterTest == 3)
StocksModel1 = glm(PositiveDec ~ ., data = stocksTrain1, family = binomial)
StocksModel2 = glm(PositiveDec ~ ., data = stocksTrain2, family = binomial)
StocksModel3 = glm(PositiveDec ~ ., data = stocksTrain3, family = binomial)
PredictTest1 = predict(StocksModel1, newdata = stocksTest1, type = "response")
table(stocksTest1$PositiveDec, PredictTest1 > 0.5)
PredictTest2 = predict(StocksModel2, newdata = stocksTest2, type = "response")
table(stocksTest2$PositiveDec, PredictTest2 > 0.5)
(388 + 757)/nrow(stocksTest2)
PredictTest3 = predict(StocksModel3, newdata = stocksTest3, type = "response")
table(stocksTest3$PositiveDec, PredictTest3 > 0.5)
(49 + 13)/nrow(stocksTest3)
AllPredictions = c(PredictTest1, PredictTest2, PredictTest3)
AllOutcomes = c(stocksTest1$PositiveDec, stocksTest2$PositiveDec, stocksTest3$PositiveDec)
table(AllOutcomes, AllPredictions > 0.5)
(467 + 1544)/nrow(AllOutcomes)
(467 + 1544)/nrow(AllOutcomes)
(467 + 1544)/length(AllOutcomes)
